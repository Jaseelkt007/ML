{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPA1DI335Im6UnlCdIHQXoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Seq2Seq_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Machine Translation using LSTMs\n",
        "### Based on the paper Sequence to Sequence Learning with Neural Network - 2014"
      ],
      "metadata": {
        "id": "nq78HS_gmvyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvWmRO2AkTFk",
        "outputId": "f33201b8-a19c-4461-b174-40b31ecbeeaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decompress the files\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en.gz"
      ],
      "metadata": {
        "id": "Gc1AlsCVygJ-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to the dataset files inside the multi30k-dataset folder in your Drive\n",
        "train_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de'\n",
        "train_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en'\n",
        "\n",
        "val_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de'\n",
        "val_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en'\n",
        "\n",
        "test_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de'\n",
        "test_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en'\n",
        "\n",
        "# Function to load data from a file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "# Load the training, validation, and test datasets\n",
        "train_ger = load_data(train_de_path)\n",
        "train_eng = load_data(train_en_path)\n",
        "\n",
        "val_ger = load_data(val_de_path)\n",
        "val_eng = load_data(val_en_path)\n",
        "\n",
        "test_ger = load_data(test_de_path)\n",
        "test_eng = load_data(test_en_path)"
      ],
      "metadata": {
        "id": "kjBCuFsWvpzp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "teW0-uiIuhoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# Load spacy tokenizers for German and English\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_ger(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Special tokens\n",
        "INIT_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "# Build vocabulary from tokenized sentences\n",
        "def build_vocab(sentences, tokenizer, min_freq=2 , max_size = 10000):\n",
        "    counter = Counter()\n",
        "    # Tokenize and count the frequency of tokens\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence)\n",
        "        counter.update(tokens)\n",
        "\n",
        "    sorted_tokens = sorted(counter.items() , key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    # Build a vocab from words appearing more than min_freq times\n",
        "    vocab = {word: i+4 for i, (word, count) in enumerate(sorted_tokens[:max_size]) if count >= min_freq}\n",
        "    # special tokens\n",
        "    vocab[INIT_TOKEN] = 0\n",
        "    vocab[EOS_TOKEN] = 1\n",
        "    vocab[PAD_TOKEN] = 2\n",
        "    vocab[UNK_TOKEN] = 3\n",
        "    return vocab\n",
        "\n",
        "# Build vocab for both source and target sentences\n",
        "german_vocab = build_vocab(train_ger, tokenize_ger)\n",
        "english_vocab = build_vocab(train_eng, tokenize_eng)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_tokens = [INIT_TOKEN] + self.src_tokenizer(self.src_sentences[idx]) + [EOS_TOKEN]\n",
        "        tgt_tokens = [INIT_TOKEN] + self.tgt_tokenizer(self.tgt_sentences[idx]) + [EOS_TOKEN]\n",
        "\n",
        "        src_indices = [self.src_vocab.get(token, self.src_vocab[UNK_TOKEN]) for token in src_tokens]\n",
        "        tgt_indices = [self.tgt_vocab.get(token, self.tgt_vocab[UNK_TOKEN]) for token in tgt_tokens]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = TranslationDataset(train_ger, train_eng, german_vocab, english_vocab, tokenize_ger, tokenize_eng)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size, embedding_size, num_layers , p ) -> None:\n",
        "      super(Encoder,self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "      self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "      self.dropout = nn.Dropout(p)\n",
        "      self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # x shape : (seq_length, N)\n",
        "\n",
        "      embedding = self.dropout(self.embedding(x))\n",
        "      # embedding shape : (seq_length, N , embedding_size)\n",
        "\n",
        "      output , (hidden , cell) = self.rnn(embedding)\n",
        "      return hidden, cell # we only interested in final hidden and cell state\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_size, embedding_size, hidden_size, output_size, num_layers, p ) -> None:\n",
        "      super(Encoder, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "\n",
        "      self.dropout = nn.Dropout(p)\n",
        "      self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "      self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers) # the input size to the decoder is the size of embedding size from encoder\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x , hidden, cell):\n",
        "      # shape of x : (N) but we want (1,N)\n",
        "      x = x.unsqueeze(0)\n",
        "\n",
        "      embedding = self.dropout(self.embedding(x))\n",
        "      # embedding shape : (1 , N, embedding_size)\n",
        "\n",
        "      output , (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "      # output shape : (1, N , hidden_size)\n",
        "\n",
        "      predictions = self.fc(output)\n",
        "      # shape of predictions : (1 , N , length_of_vocab)\n",
        "\n",
        "      predictions = predictions.squeeze(0) # because we want to add the prediction at last for the final answer\n",
        "\n",
        "      return predictions, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder) -> None:\n",
        "      super(Seq2Seq,self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "      batch_size = source.shape[1]\n",
        "      target_len = target.shape[0]\n",
        "      target_vocab_size = len(english_vocab)\n",
        "\n",
        "      outputs = torch.zeros(target_len, batch_size, target_vocab_size) # length of target sequence = no of words in that sentence , number of sequences, size of target vocab\n",
        "\n",
        "      hidden , cell = self.encoder(source)\n",
        "\n",
        "      x = target[0] # first input to the decoder , is <sos>\n",
        "\n",
        "      for t in range (1, target_len):\n",
        "          output , hidden, cell = self.decoder(x , hidden, cell) # output --> (batch_size, target_vocab_size)\n",
        "\n",
        "          outputs[t] = output\n",
        "\n",
        "          best_guess = output.argmax(1)\n",
        "\n",
        "          x = target[t] if random.random() < teacher_force_ratio else best_guess # random.random() generates random no btw 0 and 1\n",
        "\n",
        "          return outputs\n",
        "\n",
        "# Traning .. to be continued\n"
      ],
      "metadata": {
        "id": "9MgsSobQcJEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}