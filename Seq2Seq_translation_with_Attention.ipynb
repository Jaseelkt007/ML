{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOvzLuV5g44zqxWa9okpoYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Seq2Seq_translation_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Machine Translation using Attention Mechanism\n",
        "#### Based on the paper Neural Machine Translation by Jointly Learning to align and Translate - 2016\n",
        "####  This paper introduces the attention mechanism which alows model to dynamicaly focus on different parts of the source sentence at each time step, rather than relying on single fixed size context vector 2014 paper.\n",
        "#### the model learn to align parts of the input sentence to corresponding parts of the output sentence.\n",
        "#### Training: Attention weights are calculated from hidden states from decoder after every prediction with the encoder states at each time steps in prediction side (these scores are calculated using Forward neural network), then these weights are element wise multiplied and summed with corresponding encoder states to form the context vector. these convext vector is concatinated with target input vector of decoder to decoder LSTM to predict next word and this process continues till end of word"
      ],
      "metadata": {
        "id": "nq78HS_gmvyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvWmRO2AkTFk",
        "outputId": "2d84e9ff-adf0-41fc-ebc5-e6fde3eda42f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decompress the files\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de.gz\n",
        "!gunzip /content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en.gz"
      ],
      "metadata": {
        "id": "Gc1AlsCVygJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to the dataset files inside the multi30k-dataset folder in your Drive\n",
        "train_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de'\n",
        "train_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en'\n",
        "\n",
        "val_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de'\n",
        "val_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en'\n",
        "\n",
        "test_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de'\n",
        "test_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en'\n",
        "\n",
        "# Function to load data from a file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "# Load the training, validation, and test datasets\n",
        "train_ger = load_data(train_de_path)\n",
        "train_eng = load_data(train_en_path)\n",
        "\n",
        "val_ger = load_data(val_de_path)\n",
        "val_eng = load_data(val_en_path)\n",
        "\n",
        "test_ger = load_data(test_de_path)\n",
        "test_eng = load_data(test_en_path)"
      ],
      "metadata": {
        "id": "kjBCuFsWvpzp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "teW0-uiIuhoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Ensure you have downloaded the required NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def bleu(data, model, german_vocab, english_vocab, device, max_length=50):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    # data should be a list of (source_sentence, target_sentence) pairs\n",
        "    for src, trg in data:\n",
        "        # src: German sentence (input)\n",
        "        # trg: English sentence (target)\n",
        "\n",
        "        # Translate the German source sentence to English using the model\n",
        "        prediction = translate_sentence(model, src, german_vocab, english_vocab, device, max_length=max_length)\n",
        "\n",
        "        # Remove <eos> token from the predicted sentence\n",
        "        prediction = prediction[:-1] if prediction[-1] == '<eos>' else prediction\n",
        "\n",
        "        # Tokenize the target sentence if it's not already tokenized\n",
        "        if isinstance(trg, str):\n",
        "            trg = nltk.word_tokenize(trg.lower())\n",
        "\n",
        "        # Add to outputs and targets\n",
        "        outputs.append(prediction)  # Predicted output\n",
        "        targets.append([trg])  # Target needs to be in a nested list for BLEU score\n",
        "\n",
        "    # Calculate BLEU score using NLTK\n",
        "    smooth = SmoothingFunction().method4  # Helps to handle short sentences and BLEU-0 cases\n",
        "    bleu_scores = [\n",
        "        sentence_bleu(target, output, smoothing_function=smooth) for target, output in zip(targets, outputs)\n",
        "    ]\n",
        "\n",
        "    # Return the average BLEU score\n",
        "    return sum(bleu_scores) / len(bleu_scores)\n"
      ],
      "metadata": {
        "id": "9MgsSobQcJEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56a2dc7-41e5-476e-d11f-9be8ee33d2c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import os\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Load spacy tokenizers for German and English\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_ger(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Special tokens\n",
        "INIT_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "def translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50): # for inference\n",
        "    # Tokenize the input sentence = german\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token.lower() for token in tokenize_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [ token.lower() for token in sentence] # if the sentence is already is tokenized\n",
        "\n",
        "    # add <sos> and <eos>\n",
        "    tokens.insert(0,'<sos>')\n",
        "    tokens.append('<eos>')\n",
        "\n",
        "    # Convert the tokens in to corresponding indices from vocab dictionary\n",
        "    text_to_token = [german_vocab.get(token, german_vocab['<unk>']) for token in tokens]\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    sentence_tensor = torch.LongTensor(text_to_token).unsqueeze(1).to(device)\n",
        "\n",
        "    # Get the hidden and cell state from encoder\n",
        "    with torch.no_grad():\n",
        "        encoder_states ,hidden , cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    # Initiliaze the outputs for decoder with <sos>\n",
        "    outputs = [english_vocab['<sos>']]\n",
        "\n",
        "\n",
        "    # Get the Decoder output for prediction for all the input words or max length\n",
        "    for _ in range(max_length):\n",
        "        previous_words = torch.LongTensor([outputs[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden , cell = model.decoder(previous_words , encoder_states , hidden , cell)\n",
        "        best_guess = output.argmax(1).item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Covert the output indices back to words\n",
        "    translated_sentence = [english_itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:] # except <sos>\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # unzip the batch into seperate the source and target sequence\n",
        "    source_batch, target_batch = zip(*batch)\n",
        "\n",
        "    # Convert lists of sequence into padded tensors\n",
        "    source_padded = pad_sequence(source_batch, padding_value=pad_idx , batch_first=False)\n",
        "    target_padded = pad_sequence(target_batch, padding_value=pad_idx , batch_first=False)\n",
        "    return source_padded, target_padded\n",
        "\n",
        "# Build vocabulary from tokenized sentences\n",
        "def build_vocab(sentences, tokenizer, min_freq=2 , max_size = 10000):\n",
        "    counter = Counter()\n",
        "    # Tokenize and count the frequency of tokens\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence)\n",
        "        counter.update(tokens)\n",
        "\n",
        "    sorted_tokens = sorted(counter.items() , key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    # Build a vocab from words appearing more than min_freq times\n",
        "    vocab = {word: i+4 for i, (word, count) in enumerate(sorted_tokens[:max_size]) if count >= min_freq}\n",
        "    # special tokens\n",
        "    vocab[INIT_TOKEN] = 0\n",
        "    vocab[EOS_TOKEN] = 1\n",
        "    vocab[PAD_TOKEN] = 2\n",
        "    vocab[UNK_TOKEN] = 3\n",
        "    return vocab\n",
        "\n",
        "# Build vocab for both source and target sentences\n",
        "german_vocab = build_vocab(train_ger, tokenize_ger)\n",
        "english_vocab = build_vocab(train_eng, tokenize_eng)\n",
        "\n",
        "# Reverse vocab (index to string)\n",
        "german_itos = {idx : word for word , idx in german_vocab.items()}\n",
        "english_itos = {idx : word for word , idx in english_vocab.items()}\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_tokens = [INIT_TOKEN] + self.src_tokenizer(self.src_sentences[idx]) + [EOS_TOKEN]\n",
        "        tgt_tokens = [INIT_TOKEN] + self.tgt_tokenizer(self.tgt_sentences[idx]) + [EOS_TOKEN]\n",
        "\n",
        "        src_indices = [self.src_vocab.get(token, self.src_vocab[UNK_TOKEN]) for token in src_tokens]\n",
        "        tgt_indices = [self.tgt_vocab.get(token, self.tgt_vocab[UNK_TOKEN]) for token in tgt_tokens]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size, embedding_size, num_layers , p ) -> None:\n",
        "      super(Encoder,self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "      self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "      self.dropout = nn.Dropout(p)\n",
        "      self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True , dropout=p)\n",
        "      self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "      self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "      # x shape : (seq_length, N)\n",
        "\n",
        "      embedding = self.dropout(self.embedding(x))\n",
        "      # embedding shape : (seq_length, N , embedding_size)\n",
        "\n",
        "      encoder_states , (hidden , cell) = self.rnn(embedding)\n",
        "\n",
        "      # final hidden state shape in BiLSTM : (2 * num_layers , N , hidden_size)\n",
        "      hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2)) # concatinated forward and backward,along hidden size axis , because decoder expect (1,N,1024) = num_layers, batch,hidden_size\n",
        "\n",
        "      cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "      return encoder_states , hidden, cell # shape of encoder states : (sequence_length, N , hidden_size * 2), it contain hidden states of all tokens\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_size, embedding_size, hidden_size, output_size, num_layers, p ) -> None:\n",
        "      super(Decoder, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "\n",
        "      self.dropout = nn.Dropout(p)\n",
        "      self.embedding = nn.Embedding(input_size, embedding_size) # (1 , N , embedding_size)\n",
        "      self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size ,num_layers) # the input size_encoder = embedding size  + context vector\n",
        "\n",
        "      self.energy = nn.Linear(hidden_size*3,1)\n",
        "      self.softmax = nn.Softmax(dim=0)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x ,encoder_states, hidden, cell):\n",
        "      # shape of x : (N) but we want (1,N)\n",
        "      x = x.unsqueeze(0)\n",
        "\n",
        "      embedding = self.dropout(self.embedding(x))\n",
        "      # embedding shape : (1 , N, embedding_size)\n",
        "\n",
        "      sequence_length = encoder_states.shape[0] # encoder_shape -> (sequence_length, N , hidden_size * 2)\n",
        "      h_reshaped = hidden.repeat(sequence_length, 1, 1) # hidden state decoder - (1 , N, hidden_size) ---> (sequence_length, N, hidden_size)\n",
        "      energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2))) # resulting concatinated shape shape => (sequence_lengeth, N, hidden_size * 3)\n",
        "      attention = self.softmax(energy) # (sequence_length, N ,1 )\n",
        "\n",
        "      attention = attention.permute(1, 2, 0) # (N, 1, sequence_length)\n",
        "      encoder_states = encoder_states.permute(1,0,2) # (N, sequence_length, hidden_size *2) # many operation in pytorch expects Batch in 0th dimension\n",
        "      context_vector = torch.bmm(attention , encoder_states).permute(1,0,2) # (N, 1, hidden_size *2) --> (1, N, hidden_size*2)\n",
        "      rnn_input = torch.cat((context_vector, embedding), dim= 2) # (1, N , hidden_size *2 + embedding_size)\n",
        "\n",
        "\n",
        "\n",
        "      output , (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "      # output shape : (1, N , hidden_size)\n",
        "\n",
        "      predictions = self.fc(output)\n",
        "      # shape of predictions : (1 , N , length_of_vocab)\n",
        "\n",
        "      predictions = predictions.squeeze(0) # because we want to add the prediction at last for the final answer\n",
        "\n",
        "      return predictions, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder) -> None:\n",
        "      super(Seq2Seq,self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "      batch_size = source.shape[1]\n",
        "      target_len = target.shape[0]\n",
        "      target_vocab_size = len(english_vocab)\n",
        "\n",
        "      outputs = torch.zeros(target_len, batch_size, target_vocab_size) # length of target sequence = no of words in that sentence , number of sequences, size of target vocab\n",
        "\n",
        "      encoder_states , hidden , cell = self.encoder(source)\n",
        "\n",
        "      x = target[0] # first input to the decoder , is\n",
        "\n",
        "      for t in range (1, target_len):\n",
        "          output , hidden, cell = self.decoder(x , encoder_states ,hidden, cell) # output --> (batch_size, target_vocab_size)\n",
        "\n",
        "          outputs[t] = output\n",
        "\n",
        "          best_guess = output.argmax(1)\n",
        "          ''' Teacher forcing is used so that model needs to trained with actual target word and predicted. In paper, teacher forcing ratio depits which one is used\n",
        "              its better to use ratio smaller in the beginnig so that during initial part of training the model learns with the actual target inputs ,\n",
        "              because the predicted words in the beginning may not good for predicting other word, but in this code, we just used random ratio for simplicity,\n",
        "              so the input to decoder can be either target or predicted word from the decoder\n",
        "          '''\n",
        "          x = target[t] if random.random() < teacher_force_ratio else best_guess # random.random() generates random no btw 0 and 1\n",
        "\n",
        "      return outputs\n",
        "\n",
        "pad_idx = english_vocab['<pad>'] # for padding to match the lenght of longest sequence in the batch\n",
        "# Hyperparameters\n",
        "num_layers= 1\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
        "input_size_encoder = len(german_vocab)\n",
        "input_size_decoder = len(english_vocab)\n",
        "output_size = len(english_vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "writer = SummaryWriter(f'runs/loss_plot')\n",
        "step=0\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = TranslationDataset(train_ger, train_eng, german_vocab, english_vocab, tokenize_ger, tokenize_eng)\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "encoder_net = Encoder(input_size_encoder, hidden_size, encoder_embedding_size, num_layers, enc_dropout).to(device)\n",
        "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
        "model = Seq2Seq(encoder_net, decoder_net)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "\n",
        "# Training\n",
        "# Define checkpoint path to save the checkpoint(each epoch)\n",
        "checkpoint_path = '/content/drive/MyDrive/multi30k_data/model_checkpoint.pth'\n",
        "\n",
        "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\" # for inference\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
        "    torch.save(checkpoint ,checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50)\n",
        "    print(f\"the Translated example sentences \\n {translated_sentence} \")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (inp_data, target) in enumerate(train_loader):\n",
        "      inp_data = inp_data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      output = model(inp_data, target) # (target_len, batch_size, output_dim)\n",
        "\n",
        "      # Reshape for loss function , loss function (N,C) --> N : target-1 * batch size\n",
        "      output = output[1:].reshape(-1, output.shape[2])\n",
        "      target = target[1:].reshape(-1) # target_len -1 * batch_size (N)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(output.to(device), target.to(device))\n",
        "      loss.backward()\n",
        "\n",
        "      # to avoid exploding grading\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "      writer.add_scalar('Training Loss ', loss.item() , global_step = batch_idx)\n",
        "\n",
        "\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "AiGPJfyyGJHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `test_data` is a list of (src_sentence, trg_sentence) pairs\n",
        "\n",
        "# Compute the BLEU score for the first 100 sentences in the test data\n",
        "score = bleu(test_data[1:100], model, german_vocab, english_vocab, device)\n",
        "\n",
        "# Print the BLEU score as a percentage\n",
        "print(f\"Bleu score: {score * 100:.2f}\")"
      ],
      "metadata": {
        "id": "zxwkIza4tTr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfC3GTekfNEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}