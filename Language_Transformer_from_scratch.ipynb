{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+aAohc00p52JHKc5m813r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Language_Transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Transformer from scratch\n",
        "### Based on the paper ' Attention is all you need - 2017 '"
      ],
      "metadata": {
        "id": "BkeDycFC-xAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define the paths to the dataset files inside the multi30k-dataset folder in your Drive\n",
        "train_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de'\n",
        "train_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en'\n",
        "\n",
        "val_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de'\n",
        "val_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en'\n",
        "\n",
        "test_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de'\n",
        "test_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en'\n",
        "\n",
        "# Function to load data from a file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "# Load the training, validation, and test datasets\n",
        "train_ger = load_data(train_de_path)\n",
        "train_eng = load_data(train_en_path)\n",
        "\n",
        "val_ger = load_data(val_de_path)\n",
        "val_eng = load_data(val_en_path)\n",
        "\n",
        "test_ger = load_data(test_de_path)\n",
        "test_eng = load_data(test_en_path)\n",
        "\n",
        "!pip install torch torchtext spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "YsNds0Y6sG00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4zRsa5mrBeY",
        "outputId": "4e7754cf-aeb7-4203-bea0-a15846c26843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 512)\n",
            "(2, 5, 512)\n",
            "(2, 5, 512)\n",
            "<class 'numpy.ndarray'>\n",
            "(2, 5, 512)\n",
            "(2, 5, 512)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Load spacy tokenizers for German and English\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_ger(text):\n",
        "    return ['<sos>'] + [ tok.text.lower() for tok in spacy_de.tokenizer(text)] + ['<eos>']\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return ['<sos>'] + [tok.text.lower() for tok in spacy_en.tokenizer(text)] +['<eos>']\n",
        "\n",
        "# Special tokens\n",
        "INIT_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "# Build vocabulary from tokenized sentences ---> Assign ID to each tokens\n",
        "def build_vocab(sentences, tokenizer, min_freq=2 , max_size = 10000):\n",
        "    counter = Counter()\n",
        "    # Tokenize and count the frequency of tokens\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence)\n",
        "        counter.update(tokens) # create a dictionary of key value pairs of token and its freuquency\n",
        "\n",
        "    sorted_tokens = sorted(counter.items() , key=lambda x: (-x[1], x[0])) # sort the tuples in frequency in descending order and key-token in ascending order\n",
        "\n",
        "    # Build a vocab from words appearing more than min_freq times\n",
        "    vocab = {word: i+4 for i, (word, count) in enumerate(sorted_tokens[:max_size]) if count >= min_freq}\n",
        "    # special tokens\n",
        "    vocab[INIT_TOKEN] = 0\n",
        "    vocab[EOS_TOKEN] = 1\n",
        "    vocab[PAD_TOKEN] = 2\n",
        "    vocab[UNK_TOKEN] = 3\n",
        "    return vocab\n",
        "\n",
        "# Build vocab for both source and target sentences\n",
        "german_vocab = build_vocab(train_ger, tokenize_ger)\n",
        "english_vocab = build_vocab(train_eng, tokenize_eng)\n",
        "\n",
        "# Reverse vocab (index to string)\n",
        "german_itos = {idx : word for word , idx in german_vocab.items()}\n",
        "english_itos = {idx : word for word , idx in english_vocab.items()}\n",
        "\n",
        "\n",
        "vocab_size = len(german_vocab)\n",
        "# initialize input embedding matrix\n",
        "embedding_size = 512\n",
        "batch_size = 64\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_size) *0.01 # initialized the embedding matrix from normal distribution\n",
        "\n",
        "# ensure the pad token has embeding of zeros\n",
        "pad_idx = german_vocab[PAD_TOKEN]\n",
        "embedding_matrix[pad_idx]= np.zeros(embedding_size)\n",
        "\n",
        "def get_embedding(sentence_id, embedding_matrix):\n",
        "    embedding = np.array( [embedding_matrix[token_id] for token_id in sentence_id ] ) # (seq_len, embedding_dim)\n",
        "    return embedding\n",
        "\n",
        "def pad_sentence_embedding(embedding , max_length, pad_embedding , embedding_dim ):\n",
        "    paded_embedding = np.tile(pad_embedding, (max_length, 1)) # repeat max_len times along first axis - row --> (max_len , embedding_size)\n",
        "    paded_embedding[:embedding.shape[0],:] = embedding # copy the orignial embedding to here, others will be padded.\n",
        "    return paded_embedding\n",
        "\n",
        "def positional_embedding( batch_size ,seq_len ,embedding_dim):\n",
        "    pos_enc = np.zeros((seq_len, embedding_dim))\n",
        "    for pos  in range(seq_len):\n",
        "        for i in range(embedding_dim):\n",
        "            if i % 2 == 0 :\n",
        "                pos_enc[pos, i] = np.sin(pos/(10000**( i / embedding_dim)))\n",
        "            else:\n",
        "                pos_enc[pos, i] = np.cos(pos/(10000**( i / embedding_dim)))\n",
        "\n",
        "    pos_enc_batch = np.tile(pos_enc , (batch_size , 1, 1)) # shape (batch , seq_len, embedding)\n",
        "\n",
        "    return pos_enc_batch\n",
        "\n",
        "\n",
        "def get_batch_embedding(batch_sentence_id, embedding_matrix):\n",
        "\n",
        "    max_len = max([len(sentence_id) for sentence_id in batch_sentence_id])\n",
        "    pad_embedding = embedding_matrix[pad_idx]\n",
        "    batch_embedding = [ pad_sentence_embedding(get_embedding(sentence_id, embedding_matrix), max_len, pad_embedding ,embedding_size) for sentence_id in batch_sentence_id]\n",
        "    pos_enc_batch = positional_embedding(len(batch_sentence_id), max_len, embedding_size )\n",
        "    batch_embedding_with_pos = batch_embedding + pos_enc_batch\n",
        "    return np.array(batch_embedding_with_pos) # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "sentence_id = [english_vocab[INIT_TOKEN],english_vocab['hello'], english_vocab['world']] # for decoder, preapend with SOS and remove eos at end\n",
        "# it ensure the decoder learns to predict the next token based on next token\n",
        "embedding = get_embedding(sentence_id, embedding_matrix) # (seq_len, embedding_size)\n",
        "print(embedding.shape)\n",
        "\n",
        "batch_of_sentence_ids = [\n",
        "    [english_vocab[INIT_TOKEN],english_vocab['hello'], english_vocab['world'], english_vocab[EOS_TOKEN]],\n",
        "    [english_vocab['this'], english_vocab['is'], english_vocab['a'], english_vocab['test'], english_vocab[EOS_TOKEN]]]\n",
        "\n",
        "batch_embedding_pos = get_batch_embedding(batch_of_sentence_ids,embedding_matrix )\n",
        "print(batch_embedding_pos.shape)\n",
        "\n",
        "\n",
        "# MULTI HEAD SELF ATTENTION\n",
        "\n",
        "def softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x , axis = -1 , keepdims=True))\n",
        "  return exp_x / np.sum(exp_x, axis=-1 , keepdims=True)\n",
        "\n",
        "''' the idea is to allow the model to focus on different parts of the sequence by using multiple heads, this allows parallel processing,\n",
        "    reduce computational cost cause each head works on smaller space\n",
        "'''\n",
        "\n",
        "def multi_head_attention(embedding_pos,embedding_dim , num_head = 8):\n",
        "\n",
        "    head_size = embedding_dim // num_head\n",
        "    # Initialize the wieght matrices for Q, K , V\n",
        "    Wq = np.random.randn(num_head,embedding_dim, head_size) # (dim, head_size)\n",
        "    Wk = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    Wv = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    all_head_outputs = []\n",
        "    # batch matrix multiplication\n",
        "    for i in range(num_head):\n",
        "      Q = embedding_pos @ Wq[i] #(B , seq_len,head_size)\n",
        "      K = embedding_pos @ Wk[i]\n",
        "      V = embedding_pos @ Wv[i]\n",
        "      # scaled dot product\n",
        "      attention_score = ((Q @ K.transpose(0,2,1))/np.sqrt(head_size)) # (B , seq_len, seq_len)\n",
        "      attention_weights = softmax(attention_score)\n",
        "      head_output = attention_weights @ V # (B , seq_len , head_size)\n",
        "      all_head_outputs.append(head_output)\n",
        "    concatenated_heads = np.concatenate(all_head_outputs, axis= -1) # (B , seq_len , embedding_size)\n",
        "    Wh = np.random.randn(embedding_dim, embedding_dim)\n",
        "    output = concatenated_heads @ Wh # (B , seq_len, embedding_size)\n",
        "    return output\n",
        "\n",
        "output = multi_head_attention(batch_embedding_pos, embedding_size)\n",
        "print(output.shape)\n",
        "\n",
        "# ADD AND NORMALIZATION\n",
        "''' Normalization is done independently for each token not across the batch or seq_len -> This helps in stabilizing the training\n",
        "    Batch Normalization - introduces dependency across samples in batch, which can interfer with parallel procesing, thats why layer normalization is used\n",
        "    Skip connection is added to reduce the vanishing gradient problems\n",
        "'''\n",
        "def add_and_norm(input ,output , epsilon = 1e-6 ):\n",
        "    added = input + output\n",
        "    mean = np.mean(added, axis=2 , keepdims=True)\n",
        "    var = np.var(added , axis = 2 , keepdims=True)\n",
        "    normalized = (added - mean)/ np.sqrt(var + epsilon)\n",
        "    # initialize the learnable parameter gamma and beta\n",
        "    gamma = np.ones((1,1,output.shape[-1]))\n",
        "    beta = np.zeros((1,1, output.shape[-1]))\n",
        "    output = gamma * normalized + beta # element wise multiplication or scaling of normalized array and shifting by bias\n",
        "    #print(output)\n",
        "    return output\n",
        "\n",
        "output_mha = add_and_norm(output , output)\n",
        "print(type(output_mha))\n",
        "print(output_mha.shape)\n",
        "def feed_forward_network(output_mha):\n",
        "    # initialise weights\n",
        "    input_dim = output_mha.shape[2] # (B , seq_len , embedding_size)\n",
        "    W1 = np.random.randn(input_dim, 4 * input_dim)\n",
        "    b1 = np.zeros((1 ,1, 4 * input_dim)) # Broadcastable bias\n",
        "    W2 = np.random.randn(4*input_dim , input_dim)\n",
        "    b2 = np.zeros((1, 1, input_dim))\n",
        "\n",
        "    z1 = output_mha @ W1 + b1 # (B , seq_len, 4 * input_dim)\n",
        "    z1 =  np.maximum(z1,0) # apply ReLU, elementwise comparison btw z1 , and 0\n",
        "    output = z1 @ W2 + b2 # (B , seq_len ,input_dim) , no Relu here\n",
        "    return output\n",
        "\n",
        "final_out = feed_forward_network(output_mha)\n",
        "print(final_out.shape)\n",
        "\n",
        "# Hyperparameter\n",
        "num_layers = 6\n",
        "num_head = 8\n",
        "batch_size = 64\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self , num_layers, embedding_dim, num_head , dropout=0.5 ,training=False) -> None:\n",
        "        self.num_layers = num_layers\n",
        "        self.emb_size = embedding_dim\n",
        "        self.num_head = num_head\n",
        "        self.dropout = dropout\n",
        "        self.training = training\n",
        "\n",
        "    def __call__(self,x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output_final = x\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            output_mh = multi_head_attention(output_final , self.emb_size , self.num_head)\n",
        "\n",
        "            # Dropout\n",
        "            if self.training:\n",
        "                dropout_mask = (np.random.rand(batch_size, seq_len,self.emb_size) >= self.dropout).astype(np.float32) # convert to float32 , True / False -> 1.0/0.0\n",
        "                output_mh *= dropout_mask\n",
        "                output_mh /= (1 - self.dropout) # scale the remaining elements to maintain the expected value of the output\n",
        "\n",
        "            output_add = add_and_norm(output_final, output_mh)\n",
        "            output_fnn = feed_forward_network(output_add)\n",
        "\n",
        "            if self.training:\n",
        "                dropout_mask = (np.random.rand(batch_size, seq_len,self.emb_size) >= self.dropout).astype(np.float32) # convert to float32 , True / False -> 1.0/0.0\n",
        "                output_fnn *= dropout_mask\n",
        "                output_fnn /= (1 - self.dropout) # scale the remaining elements to maintain the expected value of the output\n",
        "\n",
        "            output_final = add_and_norm(output_add, output_fnn)\n",
        "\n",
        "        return output_final\n",
        "\n",
        "    def train(self):\n",
        "      self.training = True # Enable Dropout\n",
        "\n",
        "    def eval(self):\n",
        "      self.training = False # No Dropout\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(num_layers, embedding_dim=embedding_size,num_head= num_head )\n",
        "encoder.eval()\n",
        "final_out = encoder(batch_embedding_pos)\n",
        "\n",
        "# Decoder Part\n",
        "''' Masking is applied to the self-attention to prevent the decoder from attending to the future tokens, this is done by setting the upper traigular values to -infinity\n",
        "    before applying softmax, which ensures that future tokens are ignored\n",
        "'''\n",
        "def create_mask(seq_len):\n",
        "    mask = np.triu(np.ones((seq_len, seq_len)), k=1) # upper triangular matrix including excluding main diagonal\n",
        "    mask = mask * (-np.inf) # convert 1s to -inifinity\n",
        "    return mask\n",
        "\n",
        "def masked_multi_head_attention(embedding_pos, embedding_dim, num_head=8):\n",
        "    head_size = embedding_dim // num_head\n",
        "    # Initialize the wieght matrices for Q, K , V\n",
        "    Wq = np.random.randn(num_head,embedding_dim, head_size) # (dim, head_size)\n",
        "    Wk = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    Wv = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    all_head_outputs = []\n",
        "    # batch matrix multiplication\n",
        "    seq_len = embedding_pos.shape[1]\n",
        "    mask = create_mask(seq_len) # (seq_len , seq_len)\n",
        "    mask = mask[np.newaxis, : , :]\n",
        "    for i in range(num_head):\n",
        "      Q = embedding_pos @ Wq[i] #(B , seq_len,head_size)\n",
        "      K = embedding_pos @ Wk[i]\n",
        "      V = embedding_pos @ Wv[i]\n",
        "      # scaled dot product\n",
        "      attention_score = ((Q @ K.transpose(0,2,1))/np.sqrt(head_size)) # (B , seq_len, seq_len)\n",
        "      masked_attention_score = attention_score + mask # create mask inside\n",
        "      attention_weights = softmax(masked_attention_score)\n",
        "      head_output = attention_weights @ V # (B , seq_len , head_size)\n",
        "      all_head_outputs.append(head_output)\n",
        "    concatenated_heads = np.concatenate(all_head_outputs, axis= -1) # (B , seq_len , embedding_size)\n",
        "    Wh = np.random.randn(embedding_dim, embedding_dim)\n",
        "    output = concatenated_heads @ Wh # (B , seq_len, embedding_size)\n",
        "    print(output.shape)\n",
        "    return output\n",
        "\n",
        "# to be continued"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_08wVDrrDxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}