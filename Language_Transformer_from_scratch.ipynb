{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO76xoYN2klCtxpuC54WTqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Language_Transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Transformer from scratch\n",
        "### Based on the paper ' Attention is all you need - 2017 '"
      ],
      "metadata": {
        "id": "BkeDycFC-xAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define the paths to the dataset files inside the multi30k-dataset folder in your Drive\n",
        "train_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.de'\n",
        "train_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/train.en'\n",
        "\n",
        "val_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.de'\n",
        "val_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/val.en'\n",
        "\n",
        "test_de_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.de'\n",
        "test_en_path = '/content/drive/MyDrive/multi30k_data/multi30k-dataset/data/task1/raw/test_2016_flickr.en'\n",
        "\n",
        "# Function to load data from a file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "# Load the training, validation, and test datasets\n",
        "train_ger = load_data(train_de_path)\n",
        "train_eng = load_data(train_en_path)\n",
        "\n",
        "val_ger = load_data(val_de_path)\n",
        "val_eng = load_data(val_en_path)\n",
        "\n",
        "test_ger = load_data(test_de_path)\n",
        "test_eng = load_data(test_en_path)\n",
        "\n",
        "!pip install torch torchtext spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "YsNds0Y6sG00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4zRsa5mrBeY",
        "outputId": "f0dda9b9-0766-446d-de81-9c190d794781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 512)\n",
            "(2, 5, 512)\n",
            "(2, 5, 512)\n",
            "[[[ 1.29833623 -1.13470109 -0.16363514]\n",
            "  [ 0.72708844 -1.41402354  0.6869351 ]\n",
            "  [ 0.68131034 -1.41390359  0.73259325]\n",
            "  [ 0.8455549  -1.40449925  0.55894435]\n",
            "  [ 0.53082878 -1.40060847  0.86977969]]\n",
            "\n",
            " [[ 1.18612732 -1.2600168   0.07388948]\n",
            "  [ 0.76959256 -1.41231536  0.6427228 ]\n",
            "  [ 0.61709645 -1.41054368  0.79344723]\n",
            "  [ 0.96645876 -1.37735918  0.41090043]\n",
            "  [ 0.54940024 -1.40324765  0.85384741]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Load spacy tokenizers for German and English\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_ger(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Special tokens\n",
        "INIT_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "# Build vocabulary from tokenized sentences ---> Assign ID to each tokens\n",
        "def build_vocab(sentences, tokenizer, min_freq=2 , max_size = 10000):\n",
        "    counter = Counter()\n",
        "    # Tokenize and count the frequency of tokens\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence)\n",
        "        counter.update(tokens) # create a dictionary of key value pairs of token and its freuquency\n",
        "\n",
        "    sorted_tokens = sorted(counter.items() , key=lambda x: (-x[1], x[0])) # sort the tuples in frequency in descending order and key-token in ascending order\n",
        "\n",
        "    # Build a vocab from words appearing more than min_freq times\n",
        "    vocab = {word: i+4 for i, (word, count) in enumerate(sorted_tokens[:max_size]) if count >= min_freq}\n",
        "    # special tokens\n",
        "    vocab[INIT_TOKEN] = 0\n",
        "    vocab[EOS_TOKEN] = 1\n",
        "    vocab[PAD_TOKEN] = 2\n",
        "    vocab[UNK_TOKEN] = 3\n",
        "    return vocab\n",
        "\n",
        "# Build vocab for both source and target sentences\n",
        "german_vocab = build_vocab(train_ger, tokenize_ger)\n",
        "english_vocab = build_vocab(train_eng, tokenize_eng)\n",
        "\n",
        "# Reverse vocab (index to string)\n",
        "german_itos = {idx : word for word , idx in german_vocab.items()}\n",
        "english_itos = {idx : word for word , idx in english_vocab.items()}\n",
        "\n",
        "\n",
        "vocab_size = len(german_vocab)\n",
        "# initialize input embedding matrix\n",
        "embedding_size = 512\n",
        "batch_size = 64\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_size) *0.01 # initialized the embedding matrix from normal distribution\n",
        "\n",
        "# ensure the pad token has embeding of zeros\n",
        "pad_idx = german_vocab[PAD_TOKEN]\n",
        "embedding_matrix[pad_idx]= np.zeros(embedding_size)\n",
        "\n",
        "def get_embedding(sentence_id, embedding_matrix):\n",
        "    embedding = np.array( [embedding_matrix[token_id] for token_id in sentence_id ] ) # (seq_len, embedding_dim)\n",
        "    return embedding\n",
        "\n",
        "def pad_sentence_embedding(embedding , max_length, pad_embedding , embedding_dim ):\n",
        "    paded_embedding = np.tile(pad_embedding, (max_length, 1)) # repeat max_len times along first axis - row --> (max_len , embedding_size)\n",
        "    paded_embedding[:embedding.shape[0],:] = embedding # copy the orignial embedding to here, others will be padded.\n",
        "    return paded_embedding\n",
        "\n",
        "def positional_embedding( batch_size ,seq_len ,embedding_dim):\n",
        "    pos_enc = np.zeros((seq_len, embedding_dim))\n",
        "    for pos  in range(seq_len):\n",
        "        for i in range(embedding_dim):\n",
        "            if i % 2 == 0 :\n",
        "                pos_enc[pos, i] = np.sin(pos/(10000**( i / embedding_dim)))\n",
        "            else:\n",
        "                pos_enc[pos, i] = np.cos(pos/(10000**( i / embedding_dim)))\n",
        "\n",
        "    pos_enc_batch = np.tile(pos_enc , (batch_size , 1, 1)) # shape (batch , seq_len, embedding)\n",
        "\n",
        "    return pos_enc_batch\n",
        "\n",
        "\n",
        "def get_batch_embedding(batch_sentence_id, embedding_matrix):\n",
        "\n",
        "    max_len = max([len(sentence_id) for sentence_id in batch_sentence_id])\n",
        "    pad_embedding = embedding_matrix[pad_idx]\n",
        "    batch_embedding = [ pad_sentence_embedding(get_embedding(sentence_id, embedding_matrix), max_len, pad_embedding ,embedding_size) for sentence_id in batch_sentence_id]\n",
        "    pos_enc_batch = positional_embedding(len(batch_sentence_id), max_len, embedding_size )\n",
        "    batch_embedding_with_pos = batch_embedding + pos_enc_batch\n",
        "    return np.array(batch_embedding_with_pos) # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "\n",
        "\n",
        "sentence_id = [english_vocab['hello'], english_vocab['world'], english_vocab[EOS_TOKEN]]\n",
        "embedding = get_embedding(sentence_id, embedding_matrix) # (seq_len, embedding_size)\n",
        "print(embedding.shape)\n",
        "\n",
        "batch_of_sentence_ids = [\n",
        "    [english_vocab[INIT_TOKEN],english_vocab['hello'], english_vocab['world'], english_vocab[EOS_TOKEN]],\n",
        "    [english_vocab['this'], english_vocab['is'], english_vocab['a'], english_vocab['test'], english_vocab[EOS_TOKEN]]]\n",
        "\n",
        "batch_embedding_pos = get_batch_embedding(batch_of_sentence_ids,embedding_matrix )\n",
        "print(batch_embedding_pos.shape)\n",
        "\n",
        "\n",
        "# MULTI HEAD SELF ATTENTION\n",
        "\n",
        "def softmax(x):\n",
        "  exp_x = np.exp(x - np.max(x , axis = -1 , keepdims=True))\n",
        "  return exp_x / np.sum(exp_x, axis=-1 , keepdims=True)\n",
        "\n",
        "''' the idea is to allow the model to focus on different parts of the sequence by using multiple heads, this allows parallel processing,\n",
        "    reduce computational cost cause each head works on smaller space\n",
        "'''\n",
        "\n",
        "def multi_head_attention(embedding_pos,embedding_dim , num_head = 8):\n",
        "\n",
        "    head_size = embedding_dim // num_head\n",
        "    # Initialize the wieght matrices for Q, K , V\n",
        "    Wq = np.random.randn(num_head,embedding_dim, head_size) # (dim, head_size)\n",
        "    Wk = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    Wv = np.random.randn(num_head,embedding_dim, head_size)\n",
        "    all_head_outputs = []\n",
        "    # batch matrix multiplication\n",
        "    for i in range(num_head):\n",
        "      Q = embedding_pos @ Wq[i] #(B , seq_len,head_size)\n",
        "      K = embedding_pos @ Wk[i]\n",
        "      V = embedding_pos @ Wv[i]\n",
        "      # scaled dot product\n",
        "      attention_score = ((Q @ K.transpose(0,2,1))/np.sqrt(head_size)) # (B , seq_len, seq_len)\n",
        "      attention_weights = softmax(attention_score)\n",
        "      head_output = attention_weights @ V # (B , seq_len , head_size)\n",
        "      all_head_outputs.append(head_output)\n",
        "    concatenated_heads = np.concatenate(all_head_outputs, axis= -1) # (B , seq_len , embedding_size)\n",
        "    Wh = np.random.randn(embedding_dim, embedding_dim)\n",
        "    output = concatenated_heads @ Wh # (B , seq_len, embedding_size)\n",
        "    return output\n",
        "\n",
        "output = multi_head_attention(batch_embedding_pos, embedding_size)\n",
        "print(output.shape)\n",
        "\n",
        "# ADD AND NORMALIZATION\n",
        "''' Normalization is done independently for each token not across the batch or seq_len -> This helps in stabilizing the training\n",
        "    Batch Normalization - introduces dependency across samples in batch, which can interfer with parallel procesing, thats layer normalization is used\n",
        "    Skip connection is added to reduce the vanishing gradient problems\n",
        "'''\n",
        "def normalization(output , epsilon = 1e-6 ):\n",
        "    mean = np.mean(output, axis=2 , keepdims=True)\n",
        "    var = np.var(output , axis = 2 , keepdims=True)\n",
        "    normalized = (output - mean)/ np.sqrt(var + epsilon)\n",
        "    # initialize the learnable parameter gamma and beta\n",
        "    gamma = np.ones((1,1,output.shape[-1]))\n",
        "    beta = np.zeros((1,1, output.shape[-1]))\n",
        "    output = gamma * normalized + beta\n",
        "    print(output)\n",
        "\n",
        "normalization(output[:,:,:3])\n",
        "\n",
        "# To be contineued"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_08wVDrrDxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}