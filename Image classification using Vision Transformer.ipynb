{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOwBjVDoRgkphRodZlN7pP6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6b508353232416abcc91827bb5e9f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61ab7b6b50ae4440825f0b054be823ec",
              "IPY_MODEL_cebbe92b3ec949028ca89e08bbc5e23a",
              "IPY_MODEL_3e1a3ac33ac14854a42102e1e24abd93"
            ],
            "layout": "IPY_MODEL_ae4d506c46384fad8c51c688d79f23a3"
          }
        },
        "61ab7b6b50ae4440825f0b054be823ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5d7451a764f4ce79bc9052baafeca1d",
            "placeholder": "​",
            "style": "IPY_MODEL_13f690a6879148e68abe6a29892e04fc",
            "value": "model.safetensors: 100%"
          }
        },
        "cebbe92b3ec949028ca89e08bbc5e23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2d5928ad05a43ffacc4a480355e4569",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9e2434bc5d44a40b31a725d91ec7374",
            "value": 346284714
          }
        },
        "3e1a3ac33ac14854a42102e1e24abd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37bcb3c2a50b4a61bb7ad84fc40cf741",
            "placeholder": "​",
            "style": "IPY_MODEL_5211c664c1284a5a9237bf574701fe10",
            "value": " 346M/346M [00:01&lt;00:00, 346MB/s]"
          }
        },
        "ae4d506c46384fad8c51c688d79f23a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d7451a764f4ce79bc9052baafeca1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f690a6879148e68abe6a29892e04fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2d5928ad05a43ffacc4a480355e4569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e2434bc5d44a40b31a725d91ec7374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37bcb3c2a50b4a61bb7ad84fc40cf741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5211c664c1284a5a9237bf574701fe10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Image%20classification%20using%20Vision%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification using Vision transformer\n",
        "### this is used classify the CIFAR10 dataset"
      ],
      "metadata": {
        "id": "Ffo1TBWHehF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "id": "teN5Zy38r73j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Basic understanding of Patch embedding and Positional embedding are follows. these functionalities are included in the pretrained models like VIT\n",
        "'''\n",
        "class Patch_embedding(nn.Module):\n",
        "    def __init__(self, img_size ,patch_size , in_channels = 3 ,embed_size = 768) -> None:\n",
        "        super().__init__()\n",
        "        self.num_pathes = (img_size // patch_size )*2\n",
        "        self.patch_size  = patch_size\n",
        "        self.proj = nn.Linear(patch_size * patch_size *in_channels , embed_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch , c , h, w = x.shape\n",
        "        # cut the images into patches of size 16*16\n",
        "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) # (batch , 3, 14,14,16,16)\n",
        "        # flatten it\n",
        "        patches = patches.contiguous().view(batch_size, -1, self.patch_size * self.patch_size * c) # ( batch, 196, 768)\n",
        "        # linear projection to embedding vector\n",
        "        embedding = self.proj(patches)\n",
        "        return embedding\n",
        "# Test the Patch Embedding\n",
        "patch_embedding = Patch_embedding(img_size=224, patch_size=16, embed_size=768)\n",
        "x = torch.randn(64, 3, 224, 224)  # Example batch of 64 images\n",
        "patch_embeddings = patch_embedding(x)  # Patch embeddings\n",
        "print(patch_embeddings.shape)  # Output shape: (64, num_patches, embedding_dim)\n",
        "\n",
        "# Positional Encodeing\n",
        "class Positional_encoding(nn.Module):\n",
        "    def __init__(self, embedding_size , max_length = 5000) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros((max_length, embedding_size))\n",
        "\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1) # (max_length, 1 ) column vector\n",
        "        # 10000^(i/d) => exp(-(1/d)*log1000) , this is computationaly efficient\n",
        "        div_term = torch.exp(torch.arange(0 , embedding_size, 2)).float() * (-math.log(10000)/embedding_size) # - this is scaling factor (embeding_len/2 ,)\n",
        "        # for even dimension\n",
        "        pe[: , 0::2] = torch.sin(position * div_term) # (max_length, embedding_size)\n",
        "        pe[: , 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (1 , max_len, embed_size)\n",
        "        self.register_buffer('pe',pe) # defined non trainable tensor(buffers), willnot be updated during training\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[: , :x.size(1)] # x.size(1) = seq_len of the image\n",
        "\n",
        "pos = Positional_encoding(embedding_size=768)\n",
        "pos_embedding = pos(patch_embeddings)\n",
        "print(pos_embedding.shape)\n",
        "\n",
        "# cls token is added to with patch embedding, then add the position embedding"
      ],
      "metadata": {
        "id": "ztXiuW4Nvu_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YfqCUv9tGtSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393,
          "referenced_widgets": [
            "d6b508353232416abcc91827bb5e9f3f",
            "61ab7b6b50ae4440825f0b054be823ec",
            "cebbe92b3ec949028ca89e08bbc5e23a",
            "3e1a3ac33ac14854a42102e1e24abd93",
            "ae4d506c46384fad8c51c688d79f23a3",
            "c5d7451a764f4ce79bc9052baafeca1d",
            "13f690a6879148e68abe6a29892e04fc",
            "a2d5928ad05a43ffacc4a480355e4569",
            "e9e2434bc5d44a40b31a725d91ec7374",
            "37bcb3c2a50b4a61bb7ad84fc40cf741",
            "5211c664c1284a5a9237bf574701fe10"
          ]
        },
        "outputId": "f8bf9eed-a7d1-422d-bd04-813b0569e27a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6b508353232416abcc91827bb5e9f3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1/10. , Loss: 0.2079, Accuracy : 94.50%\n",
            "Epoch : 2/10. , Loss: 0.0642, Accuracy : 98.16%\n",
            "Epoch : 3/10. , Loss: 0.0446, Accuracy : 98.74%\n",
            "Epoch : 4/10. , Loss: 0.0341, Accuracy : 99.07%\n",
            "Epoch : 5/10. , Loss: 0.0268, Accuracy : 99.39%\n",
            "Epoch : 6/10. , Loss: 0.0219, Accuracy : 99.41%\n",
            "Epoch : 7/10. , Loss: 0.0181, Accuracy : 99.59%\n",
            "Epoch : 8/10. , Loss: 0.0156, Accuracy : 99.67%\n",
            "Epoch : 9/10. , Loss: 0.0128, Accuracy : 99.74%\n",
            "Epoch : 10/10. , Loss: 0.0112, Accuracy : 99.82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import timm\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "\n",
        "#parameters\n",
        "image_size = 224 #226*224 - resize\n",
        "patch_size = 16 # divide image into 16*16 patches\n",
        "embedding_dim = 768 # 16*16*3\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]) # Normilise to -1 to 1\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train= True, download = True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train= False, download = True, transform = transform)\n",
        "\n",
        "# Only need to use a small (25%) subset of dataset for fine - tuning cause the domain is still the same\n",
        "train_size = len(train_dataset)\n",
        "indices = np.random.choice(train_size, size = int(train_size * 0.25), replace= False) # select 25% indices from the dataset randomly\n",
        "train_subset = Subset(train_dataset , indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size = 64, shuffle =True , num_workers=2) # (batch_size , C ,H ,W)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle=True , num_workers=2)\n",
        "\n",
        "# extract the encoder part alone including embedding.\n",
        "class VIT_Encoder(nn.Module):\n",
        "    def __init__(self, vit_model ) -> None:\n",
        "        super().__init__()\n",
        "        # Use everything except the classification head\n",
        "        self.patch_embed = vit_model.patch_embed\n",
        "        self.cls_token = vit_model.cls_token\n",
        "        self.pos_embed = vit_model.pos_embed\n",
        "        self.pos_drop = vit_model.pos_drop\n",
        "        self.blocks = vit_model.blocks # Transformer encoder layers\n",
        "        self.norm = vit_model.norm # Layer normaliztion after encoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "        # add the class token and positional embeding\n",
        "        batch_size = x.shape[0]\n",
        "        cls_token = self.cls_token.expand(batch_size, -1, -1) # multipy copies of cls token to each batches\n",
        "        x = torch.cat((cls_token , x), dim=1) # concatinate to x -> prepend to patch embedding\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x # (batch_size, seq_len, embeddin_size) --> (64, 197,768) --> 197 = 14*14(token patches) +1(cls token) ,768 = 16*16*3 is consider as the embedding size\n",
        "\n",
        "\n",
        "# classification head\n",
        "class MLP_head(nn.Module):\n",
        "    def __init__(self,embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc_out = nn.Linear(embedding_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc_out(x)\n",
        "\n",
        "\n",
        "\n",
        "class Final_model(nn.Module):\n",
        "    def __init__(self, encoder , mlp_head) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.mlp_head = mlp_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_representation = self.encoder(x) # (batch_size, seq_len, embedding_size)\n",
        "        cls_rep = out_representation[: ,0,:] # only take cls token\n",
        "        final_out = self.mlp_head(cls_rep)\n",
        "        return final_out\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 10\n",
        "embedding_dim = 768\n",
        "\n",
        "\n",
        "# Load pre-trained VIT model and extract encoder\n",
        "vit_model = timm.create_model('vit_base_patch16_224' , pretrained = True)\n",
        "vit_encoder = VIT_Encoder(vit_model)\n",
        "encoder = VIT_Encoder(vit_model)\n",
        "mlp_head = MLP_head(embedding_dim=embedding_dim,num_classes=10 )\n",
        "model = Final_model(encoder, mlp_head)\n",
        "\n",
        "# Freeze the encoder weights , to fine tune MLP alone\n",
        "for param in model.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# optimizer only update MLP parameters\n",
        "optimizer = optim.AdamW(model.mlp_head.parameters() , lr= learning_rate, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Traning\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images , labels in train_loader:\n",
        "        images , labels = images.to(device) , labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track accuracy and loss\n",
        "        running_loss += loss.item() # accumulate the batch loss\n",
        "        _ , predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item() # first compare predicted and true, then counts, then accumulate\n",
        "        total += labels.size(0) # tracks the samples processed\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100* correct/total\n",
        "    print(f'Epoch : {epoch + 1}/{num_epochs}. , Loss: {epoch_loss:.4f}, Accuracy : {epoch_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device) , labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _ , predicted = outputs.max(1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += predicted.eq(labels).sum().item()\n",
        "test_acc = 100 * test_correct/test_total\n",
        "print(f'Test Accuracy: {test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "rHeU_9XZMhk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d410bb5c-5da3-43bc-ad97-e47570099e36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the new folder path\n",
        "folder_path = '/content/drive/MyDrive/vit_models'\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Save the model\n",
        "model_save_path = os.path.join(folder_path, 'vit_finetuned.pth')\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vrM55RneJ_6",
        "outputId": "ff07a1e5-5a42-49a7-e7ed-64fe3d0fde28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to /content/drive/MyDrive/vit_models/vit_finetuned.pth\n"
          ]
        }
      ]
    }
  ]
}