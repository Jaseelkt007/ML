{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXxQ65Dur46++HqMP+Y7bp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Diabetic_Retinopathy_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ugPoHZDl-NPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73795518-4a6c-4937-e3af-74a32f575002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from multiprocessing import Pool\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "sample_data_path = '/content/drive/MyDrive/sample/test_data'\n",
        "output_folder = '/content/drive/MyDrive/sample/test_resized'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "#transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        " #   transforms.RandomRotation(20),\n",
        "  #  transforms.RandomHorizontalFlip(),\n",
        "   # transforms.ColorJitter(brightness= 0.2, contrast = 0.2),\n",
        "    #transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456,0.406], std= [0.229,0.224,0.225])\n",
        "#])\n",
        "\n",
        "def trim(image):\n",
        "\n",
        "    percentage = 0.02\n",
        "    img = np.array(image)\n",
        "    img_gray = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # Convert to grayscale to simply the process\n",
        "    # create the binary mask , to get the background from actual content\n",
        "    img_gray = img_gray > 0.1 * np.mean(img_gray[img_gray!=0])\n",
        "    # calculate the row wise and column wise sums to find where the significant content exists\n",
        "    row_sums = np.sum(img_gray, axis = 1)\n",
        "    col_sums = np.sum(img_gray, axis = 0)\n",
        "    rows = np.where(row_sums > img.shape[1] * percentage)[0] # return the rows index of rows which contain atleast 2% of its content\n",
        "    cols = np.where (col_sums > img.shape[0] * percentage)[0]\n",
        "    # find the min and max rows and columns for croping\n",
        "    min_row, min_col = np.min(rows), np.min(cols)\n",
        "    max_row, max_col = np.max(rows), np.max(cols)\n",
        "    im_crop = img[min_row : max_row +1 , min_col : max_col+1]\n",
        "    return Image.fromarray(im_crop)\n",
        "\n",
        "def resize_main_aspect(image, desired_size):\n",
        "    old_size = image.size\n",
        "    ratio = float(desired_size)/ max(old_size) # resize ratio\n",
        "    new_size = tuple([int(x * ratio) for x in old_size]) # (N,M) N,M are new size\n",
        "    im = image.resize(new_size, Image.LANCZOS) # a filter to smooth image when resize, helps to reduce artifacts in the reduced image\n",
        "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
        "    new_im.paste(im, ((desired_size - new_size[0])//2 , (desired_size - new_size[1])//2)) # paster the image on the new square background\n",
        "    return new_im\n",
        "\n",
        "def save_single(args): # helpfull for multiprocessing\n",
        "    img_file, input_path_folder, output_path_folder, output_size = args\n",
        "    image_org = Image.open(os.path.join(input_path_folder, img_file))\n",
        "    image = trim(image_org)\n",
        "    image = resize_main_aspect(image, desired_size= output_size[0])\n",
        "    image.save(os.path.join(output_path_folder , img_file))\n",
        "\n",
        "\n",
        "\n",
        "def multi_image_resize(input_path_folder, output_path_folder, output_size=None):\n",
        "    if not output_size:\n",
        "        warnings.warn(\"Need to specify output_size! For example: output_size=100\")\n",
        "        exit()\n",
        "\n",
        "    if not os.path.exists(output_path_folder):\n",
        "        os.makedirs(output_path_folder)\n",
        "\n",
        "    jobs = [\n",
        "        (file, input_path_folder, output_path_folder, output_size)\n",
        "        for file in os.listdir(input_path_folder)\n",
        "        if os.path.isfile(os.path.join(input_path_folder,file))\n",
        "    ]\n",
        "\n",
        "    with Pool() as p:\n",
        "        list(tqdm(p.imap_unordered(save_single, jobs), total=len(jobs)))\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "multi_image_resize(sample_data_path, output_folder, output_size = (256,256))\n",
        "\n",
        "\n",
        "def preprocess_images(data_path, transform):\n",
        "    processed_images = []\n",
        "    for img_name in os.listdir(data_path):\n",
        "        img_path = os.path.join(data_path, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        image = trim(image)\n",
        "        image_resized = resize_main_aspect(image, desired_size=256)\n",
        "        image = transform(image_resized)\n",
        "        processed_images.append(image)\n",
        "    return processed_images\n",
        "\n",
        "#processed_images = preprocess_images(sample_data_path, transform)\n",
        "\n",
        "def show_images(images, n=5):\n",
        "    fig, axs = plt.subplots(1, n , figsize=(15,5))\n",
        "    for i , img in enumerate(images[:n]):\n",
        "        img = img.permute(1,2,0) # change from C, H, W to H, W, C\n",
        "        img = torch.clamp(img * torch.tensor([0.229,0.224,0.225]) +\n",
        "                          torch.tensor([0.485,0.456,0.406]), 0,1) # denormalize\n",
        "        axs[i].imshow(img)\n",
        "        axs[i].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "#show_images(processed_images,n=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUTCQCOn-05P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd524f14-80e9-4d53-b3a7-d019e2a20928"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [00:59<00:00,  1.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "resized_folder = '/content/drive/MyDrive/sample/test_resized'\n",
        "final_folder = '/content/drive/MyDrive/sample/test_resized/binary'\n",
        "csv_file_path = '/content/test.csv'\n",
        "\n",
        "\n",
        "if not os.path.exists(final_folder):\n",
        "    os.makedirs(final_folder)\n",
        "\n",
        "# Create output folder structure for each class\n",
        "for i in range(2):\n",
        "    class_folder = os.path.join(final_folder, f'class_{i}')\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "with open(csv_file_path, encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    header = next(csv_reader)\n",
        "    image_name_index = header.index(\"Image name\")\n",
        "    grade_index = header.index(\"label\")\n",
        "\n",
        "    # Loop through each row in CSV and process images based on class\n",
        "    for row in tqdm(csv_reader, desc='Processing images', unit='image'):\n",
        "        image_name = row[image_name_index]\n",
        "        label = row[grade_index]\n",
        "\n",
        "        # Load the preprocessed image\n",
        "        img_path = os.path.join(resized_folder, f\"{image_name}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: {img_path} doesn't exist\")\n",
        "            continue  # Skip if the file doesn't exist\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        class_folder = os.path.join(final_folder, f'class_{label}')\n",
        "\n",
        "        # Save the original image in the respective folder\n",
        "        original_image_name = f\"{image_name}.jpg\"\n",
        "        if not os.path.exists(os.path.join(class_folder, original_image_name)):  # Save only if it doesn't exist\n",
        "            image.save(os.path.join(class_folder, original_image_name))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_m6AbkNtz55",
        "outputId": "00389b8d-1fd9-4023-cfab-e987191d4af3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 103image [00:01, 70.80image/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minor augmentation to balance the classes\n",
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define augmentation functions for minor augmentation\n",
        "def augment_image(image):  # Minor augmentation to balance the dataset\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomRotation(0.02),  # Small rotation\n",
        "        tf.keras.layers.RandomBrightness(0.05),\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    ])\n",
        "    return data_augmentation(image)\n",
        "\n",
        "# Paths and folders\n",
        "preprocessed_folder = '/content/drive/MyDrive/sample/preprocessed_samples'\n",
        "augmented_folder = '/content/drive/MyDrive/sample/aug_train_samples/binary'\n",
        "csv_file_path = '/content/train.csv'\n",
        "\n",
        "if not os.path.exists(augmented_folder):\n",
        "    os.makedirs(augmented_folder)\n",
        "\n",
        "# Create output folder structure for each class\n",
        "for i in range(2):\n",
        "    class_folder = os.path.join(augmented_folder, f'class_{i}')\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "# Set target augmentation limits (total count, including originals)\n",
        "target_augmentation = {'0': 300, '1': 50}  # Total images needed per class\n",
        "class_counters = {'0': 0, '1': 0}  # Track the count of images per class (original + augmented)\n",
        "\n",
        "# Read the CSV file and process each image\n",
        "with open(csv_file_path, encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    header = next(csv_reader)\n",
        "    image_name_index = header.index(\"Image name\")\n",
        "    grade_index = header.index(\"label\")\n",
        "\n",
        "    # Loop through each row in CSV and process images based on class\n",
        "    for row in tqdm(csv_reader, desc='Processing images', unit='image'):\n",
        "        image_name = row[image_name_index]\n",
        "        label = row[grade_index]\n",
        "        label_str = str(label)\n",
        "\n",
        "        # Load the preprocessed image\n",
        "        img_path = os.path.join(preprocessed_folder, f\"{image_name}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: {img_path} doesn't exist\")\n",
        "            continue  # Skip if the file doesn't exist\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        image_array = tf.keras.preprocessing.image.img_to_array(image)  # Convert PIL image to numpy array\n",
        "        image_array = tf.image.convert_image_dtype(image_array, dtype=tf.float32)  # Scale pixel values to [0,1]\n",
        "\n",
        "        # Define where to save images based on class label\n",
        "        class_folder = os.path.join(augmented_folder, f'class_{label}')\n",
        "\n",
        "        # Save the original image as `aug_0` for each file\n",
        "        original_image_name = f\"{image_name}_aug_0.jpg\"\n",
        "        if not os.path.exists(os.path.join(class_folder, original_image_name)):  # Save only if it doesn't exist\n",
        "            image.save(os.path.join(class_folder, original_image_name))\n",
        "            class_counters[label_str] += 1  # Count the augmented image\n",
        "\n",
        "        # Generate one augmented image as `aug_1` only if the target count is not yet met\n",
        "        if class_counters[label_str] < target_augmentation[label_str]:\n",
        "            augmented_image = augment_image(image_array)\n",
        "            augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image)  # Convert back to PIL image\n",
        "            aug_image_name = f\"{image_name}_aug_1.jpg\"\n",
        "            augmented_image.save(os.path.join(class_folder, aug_image_name))\n",
        "            class_counters[label_str] += 1  # Count the augmented image\n",
        "\n",
        "print(\"Original and augmented images saved with consistent naming.\")"
      ],
      "metadata": {
        "id": "_sHv_xiswlXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c14228-ffd3-4447-a7b3-c75014395c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 413image [00:17, 23.34image/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original and augmented images saved with consistent naming.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import logging\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomRotation(factor=0.03),  # Approximately ±10 degrees,\n",
        "        tf.keras.layers.RandomZoom(height_factor=0.1, width_factor=0.1), # small zoom\n",
        "        tf.keras.layers.RandomBrightness(0.1),\n",
        "        tf.keras.layers.RandomContrast(0.1),\n",
        "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n",
        "    ])\n",
        "\n",
        "def augment(image, label):\n",
        "    image = data_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "def preprocess(image, label, img_height=256, img_width=256):\n",
        "    \"\"\"Dataset preprocessing: Normalizing and resizing\"\"\"\n",
        "    image = tf.image.resize(image, (img_height, img_width))\n",
        "    # Normalize image to [0, 1] and resize\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    image = (image - [0.485, .456, 0.406])/ [0.229 , 0.224 ,0.225]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "def load(name, data_dir, test_data_dir, batch_size= batch_size, caching=True):\n",
        "    \"\"\"Load datasets based on name\"\"\"\n",
        "    if name == \"idrid\":\n",
        "        logging.info(f\"Preparing dataset {name}...\")\n",
        "\n",
        "        # Load dataset from directory structure, where each subdirectory represents a class,return an object, which is an iterable tuples (image, label)\n",
        "        full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            data_dir,\n",
        "            batch_size=batch_size,\n",
        "            label_mode='int' # use 'int' for integer label , for classification\n",
        "        )\n",
        "\n",
        "        # Calculate the number of examples for shuffle buffer size\n",
        "        num_examples = len(full_ds) * batch_size\n",
        "        for images, _ in full_ds.take(1):\n",
        "            image_shape = images.shape[1:]\n",
        "            break\n",
        "\n",
        "        class_names = full_ds.class_names\n",
        "        num_classes = len(class_names)\n",
        "\n",
        "        # Define df_info\n",
        "        ds_info = {\n",
        "            \"num_examples\" : num_examples,\n",
        "            \"features\" : {\n",
        "                \"image\" : {\"shape\" : image_shape , \"dtype\": tf.float32},\n",
        "                \"label\" : {\"num_classes\": num_classes, \"dtype\": tf.int64}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Split into training and validation sets\n",
        "        val_size = int(0.2 * len(full_ds))\n",
        "        train_size = len(full_ds) - val_size\n",
        "        ds_train = full_ds.take(train_size)\n",
        "        ds_val = full_ds.skip(train_size)\n",
        "\n",
        "        # Load test data\n",
        "        ds_test= None\n",
        "        if test_data_dir:\n",
        "            ds_test = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                test_data_dir, batch_size = batch_size, label_mode = 'int'\n",
        "            )\n",
        "\n",
        "        # Prepare and return the training and validation datasets\n",
        "        return prepare(ds_train, ds_val, ds_test = ds_test, ds_info = ds_info ,batch_size=batch_size, caching=caching)\n",
        "\n",
        "\n",
        "def prepare(ds_train, ds_val, ds_test= None, ds_info=None, batch_size = batch_size , caching = True):\n",
        "    \"\"\"Prepare datasets with preprocessing, augmentation, batching, caching, and prefetching\"\"\"\n",
        "    # Prepare training dataset\n",
        "    ds_train = ds_train.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if caching:\n",
        "        ds_train = ds_train.cache()\n",
        "    if ds_info:\n",
        "        shuffle_buffer_size = ds_info.get(\"num_examples\", 1000) // 10  # Default to 1000 if ds_info not provided\n",
        "        ds_train = ds_train.shuffle(shuffle_buffer_size)\n",
        "    else:\n",
        "        ds_train = ds_train.shuffle(1000)  # Fallback shuffle size\n",
        "    ds_train = ds_train.repeat().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Prepare validation dataset (no augmentation)\n",
        "    ds_val = ds_val.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if caching:\n",
        "        ds_val = ds_val.cache()\n",
        "    ds_val = ds_val.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Prepare test dataset if available (no augmentation)\n",
        "    if ds_test is not None:\n",
        "        ds_test = ds_test.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        if caching:\n",
        "            ds_test = ds_test.cache()\n",
        "        ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_train, ds_val, ds_test, ds_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/sample/aug_train_samples/binary'\n",
        "test_data_dir = '/content/drive/MyDrive/sample/test_resized/binary'\n",
        "ds_train , ds_val , _, _ = load(\"idrid\" , data_dir, test_data_dir)\n",
        "\n",
        "\n",
        "#for images, labels in ds_train.take(1):\n",
        " #   print(\"image batch shape: \", images.shape)\n",
        "  #  print(\"Label batch shape :\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irxpfjDy5K4E",
        "outputId": "27e5d97c-502a-4f97-f275-8596f8065451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 588 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, GlobalAveragePooling2D\n",
        "# Load the pretrained Model VGG16\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape = (256,256,3),\n",
        "    include_top = False,\n",
        "    weights = \"imagenet\"\n",
        ")\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024),                # Dense layer without activation\n",
        "    LeakyReLU(alpha=0.01),      # LeakyReLU with a small negative slope\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss = 'SparseCategoricalCrossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "small_ds = ds_train.take(10)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(small_ds, validation_data = ds_val , epochs=50 , verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "8uIcR-tonzn4",
        "outputId": "34822d1b-70db-4a84-8b20-5acd9425f4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ccda3f1ad6e4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the pretrained Model VGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m base_model = tf.keras.applications.VGG16(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkB6-LGxoCqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}