{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIjF+pCei9XHLJk4+UZ8rI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Diabetic_Retinopathy_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugPoHZDl-NPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240f987f-800a-45ec-ef9f-cbf689c321fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from multiprocessing import Pool\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "sample_data_path = '/content/drive/MyDrive/sample'\n",
        "output_folder = '/content/drive/MyDrive/sample/preprocessed_samples'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness= 0.2, contrast = 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456,0.406], std= [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def trim(image):\n",
        "\n",
        "    percentage = 0.02\n",
        "    img = np.array(image)\n",
        "    img_gray = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # Convert to grayscale to simply the process\n",
        "    # create the binary mask , to get the background from actual content\n",
        "    img_gray = img_gray > 0.1 * np.mean(img_gray[img_gray!=0])\n",
        "    # calculate the row wise and column wise sums to find where the significant content exists\n",
        "    row_sums = np.sum(img_gray, axis = 1)\n",
        "    col_sums = np.sum(img_gray, axis = 0)\n",
        "    rows = np.where(row_sums > img.shape[1] * percentage)[0] # return the rows index of rows which contain atleast 2% of its content\n",
        "    cols = np.where (col_sums > img.shape[0] * percentage)[0]\n",
        "    # find the min and max rows and columns for croping\n",
        "    min_row, min_col = np.min(rows), np.min(cols)\n",
        "    max_row, max_col = np.max(rows), np.max(cols)\n",
        "    im_crop = img[min_row : max_row +1 , min_col : max_col+1]\n",
        "    return Image.fromarray(im_crop)\n",
        "\n",
        "def resize_main_aspect(image, desired_size):\n",
        "    old_size = image.size\n",
        "    ratio = float(desired_size)/ max(old_size) # resize ratio\n",
        "    new_size = tuple([int(x * ratio) for x in old_size]) # (N,M) N,M are new size\n",
        "    im = image.resize(new_size, Image.LANCZOS) # a filter to smooth image when resize, helps to reduce artifacts in the reduced image\n",
        "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
        "    new_im.paste(im, ((desired_size - new_size[0])//2 , (desired_size - new_size[1])//2)) # paster the image on the new square background\n",
        "    return new_im\n",
        "\n",
        "def save_single(args): # helpfull for multiprocessing\n",
        "    img_file, input_path_folder, output_path_folder, output_size = args\n",
        "    image_org = Image.open(os.path.join(input_path_folder, img_file))\n",
        "    image = trim(image_org)\n",
        "    image = resize_main_aspect(image, desired_size= output_size[0])\n",
        "    image.save(os.path.join(output_path_folder , img_file))\n",
        "\n",
        "\n",
        "\n",
        "def multi_image_resize(input_path_folder, output_path_folder, output_size=None):\n",
        "    if not output_size:\n",
        "        warnings.warn(\"Need to specify output_size! For example: output_size=100\")\n",
        "        exit()\n",
        "\n",
        "    if not os.path.exists(output_path_folder):\n",
        "        os.makedirs(output_path_folder)\n",
        "\n",
        "    jobs = [\n",
        "        (file, input_path_folder, output_path_folder, output_size)\n",
        "        for file in os.listdir(input_path_folder)\n",
        "        if os.path.isfile(os.path.join(input_path_folder,file))\n",
        "    ]\n",
        "\n",
        "    with Pool() as p:\n",
        "        list(tqdm(p.imap_unordered(save_single, jobs), total=len(jobs)))\n",
        "\n",
        "multi_image_resize(sample_data_path, output_folder, output_size = (256,256))\n",
        "\n",
        "def preprocess_images(data_path, transform):\n",
        "    processed_images = []\n",
        "    for img_name in os.listdir(data_path):\n",
        "        img_path = os.path.join(data_path, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        image = trim(image)\n",
        "        image_resized = resize_main_aspect(image, desired_size=256)\n",
        "        image = transform(image_resized)\n",
        "        processed_images.append(image)\n",
        "    return processed_images\n",
        "\n",
        "#processed_images = preprocess_images(sample_data_path, transform)\n",
        "\n",
        "def show_images(images, n=5):\n",
        "    fig, axs = plt.subplots(1, n , figsize=(15,5))\n",
        "    for i , img in enumerate(images[:n]):\n",
        "        img = img.permute(1,2,0) # change from C, H, W to H, W, C\n",
        "        img = torch.clamp(img * torch.tensor([0.229,0.224,0.225]) +\n",
        "                          torch.tensor([0.485,0.456,0.406]), 0,1) # denormalize\n",
        "        axs[i].imshow(img)\n",
        "        axs[i].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "#show_images(processed_images,n=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUTCQCOn-05P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276f9f26-0079-4331-d4ef-130f042f1f2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  2.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRkQqtOdkZp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7BUFjMbkZym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}