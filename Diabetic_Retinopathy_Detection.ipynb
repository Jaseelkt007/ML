{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsx8pswbXaq6KNnt3Jz39R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/ML/blob/master/Diabetic_Retinopathy_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ugPoHZDl-NPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a7b8be-c64d-44a3-a25f-fba3eba442c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from multiprocessing import Pool\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "sample_data_path = '/content/drive/MyDrive/sample'\n",
        "output_folder = '/content/drive/MyDrive/sample/preprocessed_samples'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Resize((256,256)),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness= 0.2, contrast = 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456,0.406], std= [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def trim(image):\n",
        "\n",
        "    percentage = 0.02\n",
        "    img = np.array(image)\n",
        "    img_gray = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # Convert to grayscale to simply the process\n",
        "    # create the binary mask , to get the background from actual content\n",
        "    img_gray = img_gray > 0.1 * np.mean(img_gray[img_gray!=0])\n",
        "    # calculate the row wise and column wise sums to find where the significant content exists\n",
        "    row_sums = np.sum(img_gray, axis = 1)\n",
        "    col_sums = np.sum(img_gray, axis = 0)\n",
        "    rows = np.where(row_sums > img.shape[1] * percentage)[0] # return the rows index of rows which contain atleast 2% of its content\n",
        "    cols = np.where (col_sums > img.shape[0] * percentage)[0]\n",
        "    # find the min and max rows and columns for croping\n",
        "    min_row, min_col = np.min(rows), np.min(cols)\n",
        "    max_row, max_col = np.max(rows), np.max(cols)\n",
        "    im_crop = img[min_row : max_row +1 , min_col : max_col+1]\n",
        "    return Image.fromarray(im_crop)\n",
        "\n",
        "def resize_main_aspect(image, desired_size):\n",
        "    old_size = image.size\n",
        "    ratio = float(desired_size)/ max(old_size) # resize ratio\n",
        "    new_size = tuple([int(x * ratio) for x in old_size]) # (N,M) N,M are new size\n",
        "    im = image.resize(new_size, Image.LANCZOS) # a filter to smooth image when resize, helps to reduce artifacts in the reduced image\n",
        "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
        "    new_im.paste(im, ((desired_size - new_size[0])//2 , (desired_size - new_size[1])//2)) # paster the image on the new square background\n",
        "    return new_im\n",
        "\n",
        "def save_single(args): # helpfull for multiprocessing\n",
        "    img_file, input_path_folder, output_path_folder, output_size = args\n",
        "    image_org = Image.open(os.path.join(input_path_folder, img_file))\n",
        "    image = trim(image_org)\n",
        "    image = resize_main_aspect(image, desired_size= output_size[0])\n",
        "    image.save(os.path.join(output_path_folder , img_file))\n",
        "\n",
        "\n",
        "\n",
        "def multi_image_resize(input_path_folder, output_path_folder, output_size=None):\n",
        "    if not output_size:\n",
        "        warnings.warn(\"Need to specify output_size! For example: output_size=100\")\n",
        "        exit()\n",
        "\n",
        "    if not os.path.exists(output_path_folder):\n",
        "        os.makedirs(output_path_folder)\n",
        "\n",
        "    jobs = [\n",
        "        (file, input_path_folder, output_path_folder, output_size)\n",
        "        for file in os.listdir(input_path_folder)\n",
        "        if os.path.isfile(os.path.join(input_path_folder,file))\n",
        "    ]\n",
        "\n",
        "    with Pool() as p:\n",
        "        list(tqdm(p.imap_unordered(save_single, jobs), total=len(jobs)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #multi_image_resize(sample_data_path, output_folder, output_size = (256,256))\n",
        "    pass\n",
        "\n",
        "def preprocess_images(data_path, transform):\n",
        "    processed_images = []\n",
        "    for img_name in os.listdir(data_path):\n",
        "        img_path = os.path.join(data_path, img_name)\n",
        "        image = Image.open(img_path)\n",
        "        image = trim(image)\n",
        "        image_resized = resize_main_aspect(image, desired_size=256)\n",
        "        image = transform(image_resized)\n",
        "        processed_images.append(image)\n",
        "    return processed_images\n",
        "\n",
        "#processed_images = preprocess_images(sample_data_path, transform)\n",
        "\n",
        "def show_images(images, n=5):\n",
        "    fig, axs = plt.subplots(1, n , figsize=(15,5))\n",
        "    for i , img in enumerate(images[:n]):\n",
        "        img = img.permute(1,2,0) # change from C, H, W to H, W, C\n",
        "        img = torch.clamp(img * torch.tensor([0.229,0.224,0.225]) +\n",
        "                          torch.tensor([0.485,0.456,0.406]), 0,1) # denormalize\n",
        "        axs[i].imshow(img)\n",
        "        axs[i].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "#show_images(processed_images,n=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LUTCQCOn-05P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c7149c-bd17-4e56-bc80-13500e47e89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 413/413 [04:01<00:00,  1.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness= 0.1, contrast=0.1),\n",
        "])\n",
        "\n",
        "preprocessed_folder = '/content/drive/MyDrive/sample/preprocessed_samples'\n",
        "augmented_folder = '/content/drive/MyDrive/sample/aug_train_samples'\n",
        "csv_file_path = '/content/train.csv'\n",
        "\n",
        "if not os.path.exists(augmented_folder):\n",
        "    os.makedirs(augmented_folder)\n",
        "\n",
        "# create output folder structure each class\n",
        "for i in range(5):\n",
        "    class_folder = os.path.join(augmented_folder, f'class_{i}')\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "# augmentation per class:\n",
        "augmentation_counts = {\n",
        "      '0' : 2, # fewer augumentation for class 0\n",
        "      '1' : 15, # more augmentation\n",
        "      '2' : 2,\n",
        "      '3' : 4, # moderate augmentation for class 3 and 4\n",
        "      '4' : 6\n",
        "  }\n",
        "\n",
        "with open(csv_file_path, encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    header = next(csv_reader)\n",
        "    image_name_index = header.index(\"Image name\")\n",
        "    grade_index = header.index(\"Retinopathy grade\")\n",
        "\n",
        "\n",
        "    #Loop through each row in csv and augment imges based on class\n",
        "    for row in tqdm(csv_reader , desc='Augmenting images', unit='image'):\n",
        "        image_name = row[image_name_index]\n",
        "        label = row[grade_index]\n",
        "\n",
        "        #Load the preprocessed image\n",
        "        img_path = os.path.join(preprocessed_folder, f\"{image_name}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: {img_path} doesn't exist\")\n",
        "            continue # skip if the file doesn't exist\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # define where to save the image based on the class\n",
        "        class_folder = os.path.join(augmented_folder, f'class_{label}')\n",
        "        num_augmentation = augmentation_counts[label]\n",
        "\n",
        "        for i in range(num_augmentation):\n",
        "            augmented_image = augmentation_transforms(image)\n",
        "            # save the image to corresponding class folder\n",
        "            aug_image_name = f'{image_name}_aug_{i}.jpg'\n",
        "            augmented_image.save(os.path.join(class_folder, aug_image_name))\n",
        "print(\"Augmentation and saving completed\")"
      ],
      "metadata": {
        "id": "E7BUFjMbkZym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b959ed00-2942-48d7-fa8d-aaee86f0c378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting images: 413image [00:25, 16.38image/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation and saving completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the augmented folder\n",
        "augmented_folder = '/content/drive/MyDrive/sample/aug_train_samples'\n",
        "\n",
        "# Loop through each class folder and count the files\n",
        "for i in range(5):\n",
        "    class_folder = os.path.join(augmented_folder, f'class_{i}')\n",
        "    if os.path.exists(class_folder):\n",
        "        files = os.listdir(class_folder)\n",
        "        print(f\"Number of files in {class_folder}: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"{class_folder} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZB7dg8e98BJ",
        "outputId": "211fbecb-2599-4f4c-a8de-f15ccfb844f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in /content/drive/MyDrive/sample/aug_train_samples/class_0: 268\n",
            "Number of files in /content/drive/MyDrive/sample/aug_train_samples/class_1: 200\n",
            "Number of files in /content/drive/MyDrive/sample/aug_train_samples/class_2: 272\n",
            "Number of files in /content/drive/MyDrive/sample/aug_train_samples/class_3: 370\n",
            "Number of files in /content/drive/MyDrive/sample/aug_train_samples/class_4: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "TONsnSG6cP0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Define augmentation functions using TensorFlow without tensorflow-addons\n",
        "def augment_image(image):\n",
        "    # Random rotation between -20 and +20 degrees\n",
        "    angle = tf.random.uniform([], minval=-20, maxval=20, dtype=tf.float32) * (math.pi / 180.0)\n",
        "    image = tf.image.rot90(image, k=int(angle / (math.pi / 2)))  # Approximate rotation\n",
        "\n",
        "    # Random horizontal flip\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # Color jitter (brightness and contrast adjustments)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Set up paths and folders\n",
        "preprocessed_folder = '/content/drive/MyDrive/sample/preprocessed_samples'\n",
        "augmented_folder = '/content/drive/MyDrive/sample/aug_train_samples'\n",
        "csv_file_path = '/content/train.csv'\n",
        "\n",
        "if not os.path.exists(augmented_folder):\n",
        "    os.makedirs(augmented_folder)\n",
        "\n",
        "# Create output folder structure for each class\n",
        "for i in range(5):\n",
        "    class_folder = os.path.join(augmented_folder, f'class_{i}')\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "# Define the number of augmentations per class\n",
        "augmentation_counts = {\n",
        "    '0': 2,   # Fewer augmentations for class 0\n",
        "    '1': 10,  # More augmentations for class 1\n",
        "    '2': 2,   # Fewer augmentations for class 2\n",
        "    '3': 5,   # Moderate augmentations for class 3\n",
        "    '4': 8    # Moderate augmentations for class 4\n",
        "}\n",
        "\n",
        "# Read the CSV file and process each image\n",
        "with open(csv_file_path, encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    header = next(csv_reader)\n",
        "    image_name_index = header.index(\"Image name\")\n",
        "    grade_index = header.index(\"Retinopathy grade\")\n",
        "\n",
        "    # Loop through each row in CSV and augment images based on class\n",
        "    for row in tqdm(csv_reader, desc='Augmenting images', unit='image'):\n",
        "        image_name = row[image_name_index]\n",
        "        label = row[grade_index]\n",
        "\n",
        "        # Load the preprocessed image\n",
        "        img_path = os.path.join(preprocessed_folder, f\"{image_name}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"Warning: {img_path} doesn't exist\")\n",
        "            continue  # Skip if the file doesn't exist\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        image = tf.keras.preprocessing.image.img_to_array(image)  # Convert PIL image to numpy array\n",
        "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)  # Scale pixel values to [0,1]\n",
        "\n",
        "        # Define where to save the augmented images based on class label\n",
        "        class_folder = os.path.join(augmented_folder, f'class_{label}')\n",
        "        num_augmentation = augmentation_counts[label]\n",
        "\n",
        "        # Generate and save augmented images\n",
        "        for i in range(num_augmentation):\n",
        "            augmented_image = augment_image(image)\n",
        "            augmented_image = tf.keras.preprocessing.image.array_to_img(augmented_image)  # Convert back to PIL image\n",
        "            aug_image_name = f\"{image_name}_aug_{i}.jpg\"\n",
        "            augmented_image.save(os.path.join(class_folder, aug_image_name))\n",
        "\n",
        "print(\"Augmentation and saving completed.\")"
      ],
      "metadata": {
        "id": "_sHv_xiswlXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import logging\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomRotation(0.02), # +-2 degrees\n",
        "        tf.keras.layers.RandomZoom(0.95,1.0), # small zoom (95%-100%)\n",
        "        tf.keras.layers.RandomContrast(0.05),\n",
        "    ])\n",
        "\n",
        "def augment(image, label):\n",
        "    image = data_augmentation(image, training=True)\n",
        "    return image, label\n",
        "\n",
        "def preprocess(image, label, img_height=256, img_width=256):\n",
        "    \"\"\"Dataset preprocessing: Normalizing and resizing\"\"\"\n",
        "    image = tf.image.resize(image, (img_height, img_width))\n",
        "    # Normalize image to [0, 1] and resize\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    image = (image - [0.485, .456, 0.406])/ [0.229 , 0.224 ,0.225]\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "def load(name, data_dir, batch_size= batch_size, caching=True):\n",
        "    \"\"\"Load datasets based on name\"\"\"\n",
        "    if name == \"idrid\":\n",
        "        logging.info(f\"Preparing dataset {name}...\")\n",
        "\n",
        "        # Load dataset from directory structure, where each subdirectory represents a class,return an object, which is an iterable tuples (image, label)\n",
        "        full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            data_dir,\n",
        "            batch_size=batch_size,\n",
        "            label_mode='int' # use 'int' for integer label , for classification\n",
        "        )\n",
        "\n",
        "        # Calculate the number of examples for shuffle buffer size\n",
        "        num_examples = len(full_ds) * batch_size\n",
        "        ds_info ={ \"num_examples \" : num_examples}\n",
        "\n",
        "        # Split into training and validation sets\n",
        "        val_size = int(0.2 * len(full_ds))\n",
        "        train_size = len(full_ds) - val_size\n",
        "        ds_train = full_ds.take(train_size)\n",
        "        ds_val = full_ds.skip(train_size)\n",
        "\n",
        "        # Prepare and return the training and validation datasets\n",
        "        return prepare(ds_train, ds_val, ds_info = ds_info ,batch_size=batch_size, caching=caching)\n",
        "\n",
        "\n",
        "def prepare(ds_train, ds_val, ds_test= None, ds_info=None, batch_size = batch_size , caching = True):\n",
        "    \"\"\"Prepare datasets with preprocessing, augmentation, batching, caching, and prefetching\"\"\"\n",
        "    # Prepare training dataset\n",
        "    ds_train = ds_train.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if caching:\n",
        "        ds_train = ds_train.cache()\n",
        "    if ds_info:\n",
        "        shuffle_buffer_size = ds_info.get(\"num_examples\", 1000) // 10  # Default to 1000 if ds_info not provided\n",
        "        ds_train = ds_train.shuffle(shuffle_buffer_size)\n",
        "    else:\n",
        "        ds_train = ds_train.shuffle(1000)  # Fallback shuffle size\n",
        "    ds_train = ds_train.repeat().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Prepare validation dataset (no augmentation)\n",
        "    ds_val = ds_val.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if caching:\n",
        "        ds_val = ds_val.cache()\n",
        "    ds_val = ds_val.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Prepare test dataset if available (no augmentation)\n",
        "    if ds_test is not None:\n",
        "        ds_test = ds_test.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        if caching:\n",
        "            ds_test = ds_test.cache()\n",
        "        ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_train, ds_val, ds_test, ds_info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/sample/aug_train_samples'\n",
        "ds_train , ds_val , _, _ = load(\"idrid\" , data_dir)\n",
        "\n",
        "\n",
        "#for images, labels in ds_train.take(1):\n",
        " #   print(\"image batch shape: \", images.shape)\n",
        "  #  print(\"Label batch shape :\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irxpfjDy5K4E",
        "outputId": "96f56c1f-dd5f-47e3-9184-c91233c0f25c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1502 files belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, GlobalAveragePooling2D\n",
        "# Load the pretrained Model VGG16\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape = (256,256,3),\n",
        "    include_top = False,\n",
        "    weights = \"imagenet\"\n",
        ")\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024),                # Dense layer without activation\n",
        "    LeakyReLU(alpha=0.01),      # LeakyReLU with a small negative slope\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "small_ds = ds_train.take(5)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(small_ds, validation_data = ds_val , epochs=40 , verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uIcR-tonzn4",
        "outputId": "35b87ddd-f23f-4d75-f91e-057f90ad97e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 860ms/step - accuracy: 0.2157 - loss: 1.7114 - val_accuracy: 0.2838 - val_loss: 1.5613\n",
            "Epoch 2/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 578ms/step - accuracy: 0.2967 - loss: 1.5646 - val_accuracy: 0.2027 - val_loss: 1.5491\n",
            "Epoch 3/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 590ms/step - accuracy: 0.2552 - loss: 1.5960 - val_accuracy: 0.3108 - val_loss: 1.4352\n",
            "Epoch 4/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 594ms/step - accuracy: 0.2888 - loss: 1.5848 - val_accuracy: 0.4009 - val_loss: 1.3995\n",
            "Epoch 5/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 602ms/step - accuracy: 0.3428 - loss: 1.5009 - val_accuracy: 0.3964 - val_loss: 1.3955\n",
            "Epoch 6/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 608ms/step - accuracy: 0.3056 - loss: 1.5199 - val_accuracy: 0.4459 - val_loss: 1.3085\n",
            "Epoch 7/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609ms/step - accuracy: 0.3710 - loss: 1.4452 - val_accuracy: 0.3694 - val_loss: 1.3521\n",
            "Epoch 8/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617ms/step - accuracy: 0.3542 - loss: 1.4465 - val_accuracy: 0.4369 - val_loss: 1.2956\n",
            "Epoch 9/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 622ms/step - accuracy: 0.3291 - loss: 1.4543 - val_accuracy: 0.4640 - val_loss: 1.2589\n",
            "Epoch 10/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 613ms/step - accuracy: 0.3648 - loss: 1.3141 - val_accuracy: 0.4009 - val_loss: 1.2767\n",
            "Epoch 11/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 602ms/step - accuracy: 0.4159 - loss: 1.3448 - val_accuracy: 0.4640 - val_loss: 1.2431\n",
            "Epoch 12/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 644ms/step - accuracy: 0.3833 - loss: 1.3896 - val_accuracy: 0.4144 - val_loss: 1.2149\n",
            "Epoch 13/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 592ms/step - accuracy: 0.3876 - loss: 1.3792 - val_accuracy: 0.4054 - val_loss: 1.2220\n",
            "Epoch 14/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - accuracy: 0.4210 - loss: 1.3191 - val_accuracy: 0.4279 - val_loss: 1.2296\n",
            "Epoch 15/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - accuracy: 0.3961 - loss: 1.3538 - val_accuracy: 0.4640 - val_loss: 1.2024\n",
            "Epoch 16/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 586ms/step - accuracy: 0.3724 - loss: 1.3761 - val_accuracy: 0.4820 - val_loss: 1.1861\n",
            "Epoch 17/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 577ms/step - accuracy: 0.4140 - loss: 1.3028 - val_accuracy: 0.4910 - val_loss: 1.2031\n",
            "Epoch 18/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 624ms/step - accuracy: 0.3821 - loss: 1.3269 - val_accuracy: 0.4459 - val_loss: 1.1737\n",
            "Epoch 19/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 576ms/step - accuracy: 0.3918 - loss: 1.3149 - val_accuracy: 0.4595 - val_loss: 1.1884\n",
            "Epoch 20/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 575ms/step - accuracy: 0.4346 - loss: 1.3146 - val_accuracy: 0.4685 - val_loss: 1.1821\n",
            "Epoch 21/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 581ms/step - accuracy: 0.4140 - loss: 1.2994 - val_accuracy: 0.4099 - val_loss: 1.1840\n",
            "Epoch 22/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590ms/step - accuracy: 0.4253 - loss: 1.3177 - val_accuracy: 0.5180 - val_loss: 1.1482\n",
            "Epoch 23/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 585ms/step - accuracy: 0.4749 - loss: 1.2626 - val_accuracy: 0.4730 - val_loss: 1.1699\n",
            "Epoch 24/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - accuracy: 0.4657 - loss: 1.2780 - val_accuracy: 0.4234 - val_loss: 1.1626\n",
            "Epoch 25/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 596ms/step - accuracy: 0.4544 - loss: 1.2766 - val_accuracy: 0.4910 - val_loss: 1.1518\n",
            "Epoch 26/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 597ms/step - accuracy: 0.4454 - loss: 1.2851 - val_accuracy: 0.4505 - val_loss: 1.1428\n",
            "Epoch 27/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.4855 - loss: 1.2421 - val_accuracy: 0.3919 - val_loss: 1.1978\n",
            "Epoch 28/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.4561 - loss: 1.2281 - val_accuracy: 0.4505 - val_loss: 1.1201\n",
            "Epoch 29/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 593ms/step - accuracy: 0.4191 - loss: 1.2527 - val_accuracy: 0.5090 - val_loss: 1.1407\n",
            "Epoch 30/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.4795 - loss: 1.2024 - val_accuracy: 0.4775 - val_loss: 1.1561\n",
            "Epoch 31/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.4758 - loss: 1.2092 - val_accuracy: 0.4459 - val_loss: 1.1351\n",
            "Epoch 32/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 593ms/step - accuracy: 0.4717 - loss: 1.1728 - val_accuracy: 0.4865 - val_loss: 1.1389\n",
            "Epoch 33/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 586ms/step - accuracy: 0.5115 - loss: 1.1518 - val_accuracy: 0.5135 - val_loss: 1.1276\n",
            "Epoch 34/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.5528 - loss: 1.1845 - val_accuracy: 0.4775 - val_loss: 1.1358\n",
            "Epoch 35/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 600ms/step - accuracy: 0.4662 - loss: 1.2366 - val_accuracy: 0.4955 - val_loss: 1.1175\n",
            "Epoch 36/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 581ms/step - accuracy: 0.4522 - loss: 1.2010 - val_accuracy: 0.4775 - val_loss: 1.1074\n",
            "Epoch 37/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 579ms/step - accuracy: 0.5139 - loss: 1.1034 - val_accuracy: 0.4820 - val_loss: 1.1619\n",
            "Epoch 38/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.4538 - loss: 1.2685 - val_accuracy: 0.4550 - val_loss: 1.1300\n",
            "Epoch 39/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593ms/step - accuracy: 0.5019 - loss: 1.1401 - val_accuracy: 0.4910 - val_loss: 1.0796\n",
            "Epoch 40/40\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593ms/step - accuracy: 0.4591 - loss: 1.2399 - val_accuracy: 0.4640 - val_loss: 1.1690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkB6-LGxoCqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}